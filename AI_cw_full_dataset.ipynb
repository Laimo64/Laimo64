{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Laimo64/Laimo64/blob/main/AI_cw_full_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r38-9rAarXfj",
        "outputId": "9eb4fdbd-0265-45e5-efd5-2457871343fa"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1eSU-or72yvc3btOhfLO-edux5VBe_TrX\n",
            "From (redirected): https://drive.google.com/uc?id=1eSU-or72yvc3btOhfLO-edux5VBe_TrX&confirm=t&uuid=6d881a74-f7c5-4aec-8387-348fc626b763\n",
            "To: /content/brain.zip\n",
            "100% 4.97G/4.97G [01:06<00:00, 75.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/1eSU-or72yvc3btOhfLO-edux5VBe_TrX/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q brain.zip"
      ],
      "metadata": {
        "id": "E2w65l_6rgvu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataloader (include resize)"
      ],
      "metadata": {
        "id": "9e6EsreF4YFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "class MRCTDataset(Dataset):\n",
        "    def __init__(self, data_dir, target_size=(128, 128, 128)):  ###### target_size=(96, 96, 96)改128試試\n",
        "        \"\"\"\n",
        "        初始化數據集\n",
        "        Args:\n",
        "            data_dir (str): MRI 和 CT 數據的根目錄。\n",
        "            target_size (tuple): 將 MRI 和 CT 影像調整為的固定尺寸。\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.target_size = target_size\n",
        "        self.samples = [\n",
        "            os.path.join(root)\n",
        "            for root, _, files in os.walk(data_dir)\n",
        "            if \"mr.nii.gz\" in files and \"ct.nii.gz\" in files\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        根據索引返回一組 MR 和 CT 影像。\n",
        "        Args:\n",
        "            idx (int): 數據的索引。\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]: MR 和 CT 的張量形式。\n",
        "        \"\"\"\n",
        "        sample_path = self.samples[idx]\n",
        "\n",
        "        # 加載 MRI 和 CT 影像\n",
        "        mr = nib.load(os.path.join(sample_path, \"mr.nii.gz\")).get_fdata()\n",
        "        ct = nib.load(os.path.join(sample_path, \"ct.nii.gz\")).get_fdata()\n",
        "\n",
        "        # Z-score 標準化\n",
        "        mr = self._normalize(mr)\n",
        "        ct = self._normalize(ct)\n",
        "\n",
        "        # 調整或填充影像大小\n",
        "        mr = self._resize_or_pad(mr, self.target_size)\n",
        "        ct = self._resize_or_pad(ct, self.target_size)\n",
        "\n",
        "        # 轉換為 PyTorch 張量並增加通道維度\n",
        "        mr = torch.tensor(mr, dtype=torch.float32).unsqueeze(0)  # (1, D, H, W)\n",
        "        ct = torch.tensor(ct, dtype=torch.float32).unsqueeze(0)  # (1, D, H, W)\n",
        "\n",
        "        return mr, ct\n",
        "\n",
        "    def _normalize(self, image):\n",
        "        \"\"\"\n",
        "        Z-score 標準化影像數據。\n",
        "        Args:\n",
        "            image (np.ndarray): 輸入影像。\n",
        "        Returns:\n",
        "            np.ndarray: 標準化的影像。\n",
        "        \"\"\"\n",
        "        if np.std(image) != 0:\n",
        "            return (image - np.mean(image)) / np.std(image)\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "    def _resize_or_pad(self, image, desired_shape):\n",
        "        \"\"\"\n",
        "        調整影像大小或填充至固定大小。\n",
        "        Args:\n",
        "            image (np.ndarray): 輸入影像。\n",
        "            desired_shape (tuple): 目標大小。\n",
        "        Returns:\n",
        "            np.ndarray: 調整或填充後的影像。\n",
        "        \"\"\"\n",
        "        current_shape = image.shape\n",
        "        scale = [d / c for d, c in zip(desired_shape, current_shape)]\n",
        "        resized_image = zoom(image, scale, order=1)  # 調整大小\n",
        "\n",
        "        # 填充影像至目標大小\n",
        "        padded_image = np.zeros(desired_shape, dtype=resized_image.dtype)\n",
        "        pad_slices = tuple(slice(0, min(dim, resized_image.shape[i])) for i, dim in enumerate(desired_shape))\n",
        "        padded_image[pad_slices] = resized_image[:desired_shape[0], :desired_shape[1], :desired_shape[2]]\n",
        "\n",
        "        return padded_image\n"
      ],
      "metadata": {
        "id": "X7DjiWjf1PQA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MESP(Structure)"
      ],
      "metadata": {
        "id": "oIKPMUDJ4tUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Swin Transformer Block (簡化版，適用於3D)\n",
        "class SwinTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, input_size):\n",
        "        super(SwinTransformerBlock, self).__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fc = nn.Linear(dim, dim)\n",
        "        self.window_size = input_size // 4  # 分割窗口，根據3D的尺寸調整\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, d, h, w = x.shape  # 3D 輸入\n",
        "        x = x.view(b, -1, c)  # 展平為序列\n",
        "        x = self.norm(x)\n",
        "        x = self.fc(x)\n",
        "        return x.view(b, c, d, h, w)  # 還原為 3D\n",
        "\n",
        "# MSEP 網路\n",
        "class MSEP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MSEP, self).__init__()\n",
        "        # Encoder 部分\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv3d(1, 64, kernel_size=3, stride=1, padding=1),  # 使用3D卷積\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1),  # 使用3D卷積\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Skip 連接部分 + RDSformer\n",
        "        self.skip = SwinTransformerBlock(128, input_size=160)\n",
        "        # Decoder 部分\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose3d(128, 64, kernel_size=3, stride=1, padding=1),  # 使用3D反卷積\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose3d(64, 1, kernel_size=3, stride=1, padding=1)  # 使用3D反卷積\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc = self.encoder(x)\n",
        "        skip = self.skip(enc)  # 加入 skip connection\n",
        "        dec = self.decoder(skip)\n",
        "        return dec\n",
        "\n",
        "# Initialize model\n",
        "model = MSEP()\n",
        "\n",
        "# Test the model with dummy data (e.g., [Batch Size, Channel, Depth, Height, Width])\n",
        "dummy_input = torch.randn(1, 1, 128, 128, 128)  # 假設數據大小是 [1, 1, 128, 128, 128]\n",
        "output = model(dummy_input)\n",
        "\n",
        "print(\"Output shape:\", output.shape)  # 應該返回符合預期的 3D 輸出\n"
      ],
      "metadata": {
        "id": "3BSm9S3z4D_6",
        "outputId": "a50b0389-3cd4-40c2-ad69-70d74b322c87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 1, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training & Visualization"
      ],
      "metadata": {
        "id": "T7nNIv2z40an"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def visualize_results(input_image, target_image, predicted_image, epoch, idx):\n",
        "    \"\"\"\n",
        "    視覺化輸入影像、目標影像與預測影像\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # 輸入影像\n",
        "    axes[0].imshow(input_image[0, 0, :, :, input_image.shape[4] // 2].cpu().detach().numpy(), cmap=\"gray\")\n",
        "    axes[0].set_title(\"Input (MR)\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # 目標影像3\n",
        "    axes[1].imshow(target_image[0, 0, :, :, target_image.shape[4] // 2].cpu().detach().numpy(), cmap=\"gray\")\n",
        "    axes[1].set_title(\"Target (CT)\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    # 預測影像\n",
        "    axes[2].imshow(predicted_image[0, 0, :, :, predicted_image.shape[4] // 2].cpu().detach().numpy(), cmap=\"gray\")\n",
        "    axes[2].set_title(\"Prediction (Generated CT)\")\n",
        "    axes[2].axis(\"off\")\n",
        "\n",
        "    plt.suptitle(f\"Epoch {epoch}, Batch {idx}\")\n",
        "    plt.show()\n",
        "\n",
        "# 設置訓練設備\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 優化器與損失函數\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)  # 學習率調整\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "# 訓練參數\n",
        "epochs = 10\n",
        "best_loss = float(\"inf\")\n",
        "\n",
        "data_path = \"/content/brain\"\n",
        "output_root = \"path_to_output_folder\"  # 輸出目錄\n",
        "\n",
        "# 創建輸出資料夾\n",
        "os.makedirs(output_root, exist_ok=True)\n",
        "os.makedirs(os.path.join(output_root, \"train\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_root, \"validation\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_root, \"test\"), exist_ok=True)\n",
        "\n",
        "# 獲取所有樣本資料夾名稱\n",
        "samples = [name for name in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, name))]\n",
        "\n",
        "# 按照 70:15:15 的比例分割\n",
        "train_samples, test_samples = train_test_split(samples, test_size=0.3, random_state=42)\n",
        "validation_samples, test_samples = train_test_split(test_samples, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "# def move_samples(samples, output_dir):\n",
        "#     for sample in samples:\n",
        "#         src_path = os.path.join(data_path, sample)\n",
        "#         dst_path = os.path.join(output_dir, sample)\n",
        "#         shutil.copytree(src_path, dst_path)\n",
        "\n",
        "# # 將樣本移動到各自的資料夾\n",
        "# move_samples(train_samples, os.path.join(output_root, \"train\"))\n",
        "# move_samples(validation_samples, os.path.join(output_root, \"validation\"))\n",
        "# move_samples(test_samples, os.path.join(output_root, \"test\"))\n",
        "\n",
        "\n",
        "train_dataset = MRCTDataset(train_samples)\n",
        "validation_dataset = MRCTDataset(validation_samples)\n",
        "\n",
        "# 設置 DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(validation_dataset, batch_size=2, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# train_dataset = MRCTDataset(data_path)\n",
        "# test_dataset = SegmentationDataset(root=(data_path+'/Test'), low_res=128, transform_img=transform_img, transform_mask=transform_mask)\n",
        "# train_loader = DataLoader(train_samples, batch_size=2, shuffle=True, num_workers=2)\n",
        "# val_loader = DataLoader(test_dataset, batch_size=6, shuffle=False, num_workers=2)\n",
        "\n",
        "# for mr, ct in train_loader:\n",
        "#     print(f\"MR shape: {mr.shape}, CT shape: {ct.shape}\")\n",
        "#     break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0R4-mT2P4wMd",
        "outputId": "00806dc4-3a81-49ff-b511-043326bd1833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected str, bytes or os.PathLike object, not list",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-62977850a666>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMRCTDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mvalidation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMRCTDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9dfcba7c5583>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, target_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m         self.samples = [\n\u001b[1;32m     20\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"mr.nii.gz\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"ct.nii.gz\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         ]\n",
            "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    # 訓練階段\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "    for mr, ct in progress_bar:\n",
        "        mr, ct = mr.to(device), ct.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(mr)\n",
        "        loss = criterion(output, ct)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "    visualize_results(mr, ct, output, epoch, 1)\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Avg Train Loss: {avg_train_loss}\")\n",
        "\n",
        "    # 驗證階段\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for mr, ct in val_loader:\n",
        "            mr, ct = mr.to(device), ct.to(device)\n",
        "            output = model(mr)\n",
        "            loss = criterion(output, ct)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Avg Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "    保存最佳模型\n",
        "    if avg_val_loss < best_loss:\n",
        "        best_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(\"Best model saved!\")\n",
        "\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "hLMd16Wd5zoq",
        "outputId": "f8720426-d586-4b04-da4a-46e29773b871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   0%|          | 0/63 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"<ipython-input-3-9dfcba7c5583>\", line 47, in __getitem__\n    mr = self._resize_or_pad(mr, self.target_size)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-3-9dfcba7c5583>\", line 79, in _resize_or_pad\n    scale = [d / c for d, c in zip(desired_shape, current_shape)]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-3-9dfcba7c5583>\", line 79, in <listcomp>\n    scale = [d / c for d, c in zip(desired_shape, current_shape)]\n             ~~^~~\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-be01324afdf8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mmr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mmr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"<ipython-input-3-9dfcba7c5583>\", line 47, in __getitem__\n    mr = self._resize_or_pad(mr, self.target_size)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-3-9dfcba7c5583>\", line 79, in _resize_or_pad\n    scale = [d / c for d, c in zip(desired_shape, current_shape)]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-3-9dfcba7c5583>\", line 79, in <listcomp>\n    scale = [d / c for d, c in zip(desired_shape, current_shape)]\n             ~~^~~\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UyICao3H-Xkr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}