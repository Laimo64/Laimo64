{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Laimo64/Laimo64/blob/main/AI_cw_full_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r38-9rAarXfj",
        "outputId": "c0ec67a1-0ab4-4ea0-de59-418851aa13af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1eSU-or72yvc3btOhfLO-edux5VBe_TrX\n",
            "From (redirected): https://drive.google.com/uc?id=1eSU-or72yvc3btOhfLO-edux5VBe_TrX&confirm=t&uuid=4f862299-6967-4e0b-b7fe-6b77d4aeb341\n",
            "To: /content/brain.zip\n",
            "100% 4.97G/4.97G [01:02<00:00, 79.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/1eSU-or72yvc3btOhfLO-edux5VBe_TrX/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q brain.zip"
      ],
      "metadata": {
        "id": "E2w65l_6rgvu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataloader (include resize)"
      ],
      "metadata": {
        "id": "9e6EsreF4YFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "class MRCTDataset(Dataset):\n",
        "    def __init__(self, data_dir, target_size=(128, 128, 128)):  ###### target_size=(96, 96, 96)改128試試\n",
        "        \"\"\"\n",
        "        初始化數據集\n",
        "        Args:\n",
        "            data_dir (str): MRI 和 CT 數據的根目錄。\n",
        "            target_size (tuple): 將 MRI 和 CT 影像調整為的固定尺寸。\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.target_size = target_size\n",
        "        self.samples = [\n",
        "            os.path.join(root)\n",
        "            for root, _, files in os.walk(data_dir)\n",
        "            if \"mr.nii.gz\" in files and \"ct.nii.gz\" in files\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        根據索引返回一組 MR 和 CT 影像。\n",
        "        Args:\n",
        "            idx (int): 數據的索引。\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]: MR 和 CT 的張量形式。\n",
        "        \"\"\"\n",
        "        sample_path = self.samples[idx]\n",
        "\n",
        "        # 加載 MRI 和 CT 影像\n",
        "        mr = nib.load(os.path.join(sample_path, \"mr.nii.gz\")).get_fdata()\n",
        "        ct = nib.load(os.path.join(sample_path, \"ct.nii.gz\")).get_fdata()\n",
        "\n",
        "        # Z-score 標準化\n",
        "        mr = self._normalize(mr)\n",
        "        ct = self._normalize(ct)\n",
        "\n",
        "        # 調整或填充影像大小\n",
        "        mr = self._resize_or_pad(mr, self.target_size)\n",
        "        ct = self._resize_or_pad(ct, self.target_size)\n",
        "\n",
        "        # 轉換為 PyTorch 張量並增加通道維度\n",
        "        mr = torch.tensor(mr, dtype=torch.float32).unsqueeze(0)  # (1, D, H, W)\n",
        "        ct = torch.tensor(ct, dtype=torch.float32).unsqueeze(0)  # (1, D, H, W)\n",
        "\n",
        "        return mr, ct\n",
        "\n",
        "    def _normalize(self, image):\n",
        "        \"\"\"\n",
        "        Z-score 標準化影像數據。\n",
        "        Args:\n",
        "            image (np.ndarray): 輸入影像。\n",
        "        Returns:\n",
        "            np.ndarray: 標準化的影像。\n",
        "        \"\"\"\n",
        "        if np.std(image) != 0:\n",
        "            return (image - np.mean(image)) / np.std(image)\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "    def _resize_or_pad(self, image, desired_shape):\n",
        "        \"\"\"\n",
        "        調整影像大小或填充至固定大小。\n",
        "        Args:\n",
        "            image (np.ndarray): 輸入影像。\n",
        "            desired_shape (tuple): 目標大小。\n",
        "        Returns:\n",
        "            np.ndarray: 調整或填充後的影像。\n",
        "        \"\"\"\n",
        "        current_shape = image.shape\n",
        "        scale = [d / c for d, c in zip(desired_shape, current_shape)]\n",
        "        resized_image = zoom(image, scale, order=1)  # 調整大小\n",
        "\n",
        "        # 填充影像至目標大小\n",
        "        padded_image = np.zeros(desired_shape, dtype=resized_image.dtype)\n",
        "        pad_slices = tuple(slice(0, min(dim, resized_image.shape[i])) for i, dim in enumerate(desired_shape))\n",
        "        padded_image[pad_slices] = resized_image[:desired_shape[0], :desired_shape[1], :desired_shape[2]]\n",
        "\n",
        "        return padded_image\n"
      ],
      "metadata": {
        "id": "X7DjiWjf1PQA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Split"
      ],
      "metadata": {
        "id": "uHK1qwHESHd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "\n",
        "data_path = \"/content/brain\"\n",
        "output_root = \"/content/split\"  # 輸出目錄\n",
        "\n",
        "# 創建輸出資料夾\n",
        "os.makedirs(output_root, exist_ok=True)\n",
        "os.makedirs(os.path.join(output_root, \"train\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_root, \"validation\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_root, \"test\"), exist_ok=True)\n",
        "\n",
        "# 獲取所有樣本資料夾名稱\n",
        "samples = [name for name in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, name))]\n",
        "\n",
        "# 按照 70:15:15 的比例分割\n",
        "train_samples, test_samples = train_test_split(samples, test_size=0.3, random_state=42)\n",
        "validation_samples, test_samples = train_test_split(test_samples, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Total samples: {len(samples)}\")\n",
        "print(f\"Train samples: {len(train_samples)}, Validation samples: {len(validation_samples)}, Test samples: {len(test_samples)}\")\n",
        "\n",
        "# 定義拷貝函數\n",
        "def move_samples(samples, output_dir):\n",
        "    for sample in samples:\n",
        "        src_path = os.path.join(data_path, sample)  # 原始路徑\n",
        "        dst_path = os.path.join(output_dir, sample)  # 目標路徑\n",
        "        if os.path.exists(dst_path):\n",
        "            print(f\"Sample {sample} already exists in {output_dir}, skipping.\")\n",
        "            continue\n",
        "        shutil.copytree(src_path, dst_path)  # 拷貝整個資料夾\n",
        "        print(f\"Moved {sample} to {output_dir}\")\n",
        "\n",
        "# 將樣本移動到各自的資料夾\n",
        "move_samples(train_samples, os.path.join(output_root, \"train\"))\n",
        "move_samples(validation_samples, os.path.join(output_root, \"validation\"))\n",
        "move_samples(test_samples, os.path.join(output_root, \"test\"))\n",
        "\n",
        "print(\"Data split and moved successfully!\")\n"
      ],
      "metadata": {
        "id": "1FVKj2tISLlb",
        "outputId": "888ff4de-3006-4426-8363-e927deb78963",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 181\n",
            "Train samples: 126, Validation samples: 27, Test samples: 28\n",
            "Moved 1BC052 to /content/split/train\n",
            "Moved 1BB175 to /content/split/train\n",
            "Moved 1BA253 to /content/split/train\n",
            "Moved 1BB073 to /content/split/train\n",
            "Moved 1BB031 to /content/split/train\n",
            "Moved 1BC031 to /content/split/train\n",
            "Moved 1BC086 to /content/split/train\n",
            "Moved 1BA379 to /content/split/train\n",
            "Moved 1BB052 to /content/split/train\n",
            "Moved 1BB079 to /content/split/train\n",
            "Moved 1BB095 to /content/split/train\n",
            "Moved 1BA359 to /content/split/train\n",
            "Moved 1BC020 to /content/split/train\n",
            "Moved 1BA184 to /content/split/train\n",
            "Moved 1BB039 to /content/split/train\n",
            "Moved 1BB173 to /content/split/train\n",
            "Moved 1BC064 to /content/split/train\n",
            "Moved 1BB098 to /content/split/train\n",
            "Moved 1BA260 to /content/split/train\n",
            "Moved 1BB083 to /content/split/train\n",
            "Moved 1BB205 to /content/split/train\n",
            "Moved 1BC053 to /content/split/train\n",
            "Moved 1BA300 to /content/split/train\n",
            "Moved 1BA143 to /content/split/train\n",
            "Moved 1BA336 to /content/split/train\n",
            "Moved 1BA091 to /content/split/train\n",
            "Moved 1BA239 to /content/split/train\n",
            "Moved 1BB017 to /content/split/train\n",
            "Moved 1BC085 to /content/split/train\n",
            "Moved 1BC001 to /content/split/train\n",
            "Moved 1BA278 to /content/split/train\n",
            "Moved 1BB062 to /content/split/train\n",
            "Moved 1BA345 to /content/split/train\n",
            "Moved 1BB002 to /content/split/train\n",
            "Moved 1BC036 to /content/split/train\n",
            "Moved 1BB043 to /content/split/train\n",
            "Moved 1BB006 to /content/split/train\n",
            "Moved 1BC090 to /content/split/train\n",
            "Moved 1BC063 to /content/split/train\n",
            "Moved 1BA085 to /content/split/train\n",
            "Moved 1BA185 to /content/split/train\n",
            "Moved 1BC038 to /content/split/train\n",
            "Moved 1BC077 to /content/split/train\n",
            "Moved 1BB099 to /content/split/train\n",
            "Moved 1BC094 to /content/split/train\n",
            "Moved 1BB111 to /content/split/train\n",
            "Moved 1BA328 to /content/split/train\n",
            "Moved 1BB005 to /content/split/train\n",
            "Moved 1BC056 to /content/split/train\n",
            "Moved 1BA201 to /content/split/train\n",
            "Moved 1BA220 to /content/split/train\n",
            "Moved 1BB028 to /content/split/train\n",
            "Moved 1BA325 to /content/split/train\n",
            "Moved 1BB200 to /content/split/train\n",
            "Moved 1BC058 to /content/split/train\n",
            "Moved 1BC066 to /content/split/train\n",
            "Moved 1BA368 to /content/split/train\n",
            "Moved 1BB096 to /content/split/train\n",
            "Moved 1BB030 to /content/split/train\n",
            "Moved 1BA158 to /content/split/train\n",
            "Moved 1BC046 to /content/split/train\n",
            "Moved 1BA076 to /content/split/train\n",
            "Moved 1BA234 to /content/split/train\n",
            "Moved 1BB041 to /content/split/train\n",
            "Moved 1BB102 to /content/split/train\n",
            "Moved 1BB072 to /content/split/train\n",
            "Moved 1BA075 to /content/split/train\n",
            "Moved 1BA266 to /content/split/train\n",
            "Moved 1BC034 to /content/split/train\n",
            "Moved 1BC019 to /content/split/train\n",
            "Moved 1BC054 to /content/split/train\n",
            "Moved 1BA082 to /content/split/train\n",
            "Moved 1BC050 to /content/split/train\n",
            "Moved 1BA054 to /content/split/train\n",
            "Moved 1BC083 to /content/split/train\n",
            "Moved 1BC006 to /content/split/train\n",
            "Moved 1BA358 to /content/split/train\n",
            "Moved 1BB048 to /content/split/train\n",
            "Moved 1BA040 to /content/split/train\n",
            "Moved 1BC073 to /content/split/train\n",
            "Moved 1BB109 to /content/split/train\n",
            "Moved 1BB076 to /content/split/train\n",
            "Moved 1BA151 to /content/split/train\n",
            "Moved 1BB051 to /content/split/train\n",
            "Moved 1BB085 to /content/split/train\n",
            "Moved 1BC037 to /content/split/train\n",
            "Moved 1BA012 to /content/split/train\n",
            "Moved 1BB044 to /content/split/train\n",
            "Moved 1BC041 to /content/split/train\n",
            "Moved 1BA292 to /content/split/train\n",
            "Moved 1BB184 to /content/split/train\n",
            "Moved 1BB177 to /content/split/train\n",
            "Moved 1BB182 to /content/split/train\n",
            "Moved 1BB003 to /content/split/train\n",
            "Moved 1BB198 to /content/split/train\n",
            "Moved 1BC049 to /content/split/train\n",
            "Moved 1BB151 to /content/split/train\n",
            "Moved 1BC022 to /content/split/train\n",
            "Moved 1BA131 to /content/split/train\n",
            "Moved 1BA172 to /content/split/train\n",
            "Moved overview to /content/split/train\n",
            "Moved 1BC088 to /content/split/train\n",
            "Moved 1BB091 to /content/split/train\n",
            "Moved 1BC065 to /content/split/train\n",
            "Moved 1BA125 to /content/split/train\n",
            "Moved 1BA001 to /content/split/train\n",
            "Moved 1BC084 to /content/split/train\n",
            "Moved 1BC075 to /content/split/train\n",
            "Moved 1BC062 to /content/split/train\n",
            "Moved 1BB179 to /content/split/train\n",
            "Moved 1BB075 to /content/split/train\n",
            "Moved 1BA222 to /content/split/train\n",
            "Moved 1BA005 to /content/split/train\n",
            "Moved 1BC068 to /content/split/train\n",
            "Moved 1BA256 to /content/split/train\n",
            "Moved 1BC076 to /content/split/train\n",
            "Moved 1BA058 to /content/split/train\n",
            "Moved 1BB082 to /content/split/train\n",
            "Moved 1BB100 to /content/split/train\n",
            "Moved 1BC021 to /content/split/train\n",
            "Moved 1BC039 to /content/split/train\n",
            "Moved 1BC087 to /content/split/train\n",
            "Moved 1BA103 to /content/split/train\n",
            "Moved 1BA206 to /content/split/train\n",
            "Moved 1BC007 to /content/split/train\n",
            "Moved 1BB171 to /content/split/train\n",
            "Moved 1BA032 to /content/split/validation\n",
            "Moved 1BC010 to /content/split/validation\n",
            "Moved 1BB034 to /content/split/validation\n",
            "Moved 1BB007 to /content/split/validation\n",
            "Moved 1BC074 to /content/split/validation\n",
            "Moved 1BB066 to /content/split/validation\n",
            "Moved 1BC028 to /content/split/validation\n",
            "Moved 1BC009 to /content/split/validation\n",
            "Moved 1BA014 to /content/split/validation\n",
            "Moved 1BC080 to /content/split/validation\n",
            "Moved 1BC081 to /content/split/validation\n",
            "Moved 1BA305 to /content/split/validation\n",
            "Moved 1BA105 to /content/split/validation\n",
            "Moved 1BA111 to /content/split/validation\n",
            "Moved 1BB011 to /content/split/validation\n",
            "Moved 1BC048 to /content/split/validation\n",
            "Moved 1BC025 to /content/split/validation\n",
            "Moved 1BA116 to /content/split/validation\n",
            "Moved 1BB090 to /content/split/validation\n",
            "Moved 1BC014 to /content/split/validation\n",
            "Moved 1BB026 to /content/split/validation\n",
            "Moved 1BA164 to /content/split/validation\n",
            "Moved 1BC008 to /content/split/validation\n",
            "Moved 1BA175 to /content/split/validation\n",
            "Moved 1BB033 to /content/split/validation\n",
            "Moved 1BA227 to /content/split/validation\n",
            "Moved 1BB050 to /content/split/validation\n",
            "Moved 1BB189 to /content/split/test\n",
            "Moved 1BC017 to /content/split/test\n",
            "Moved 1BB008 to /content/split/test\n",
            "Moved 1BC027 to /content/split/test\n",
            "Moved 1BC035 to /content/split/test\n",
            "Moved 1BC023 to /content/split/test\n",
            "Moved 1BB145 to /content/split/test\n",
            "Moved 1BC051 to /content/split/test\n",
            "Moved 1BB059 to /content/split/test\n",
            "Moved 1BC070 to /content/split/test\n",
            "Moved 1BB152 to /content/split/test\n",
            "Moved 1BB071 to /content/split/test\n",
            "Moved 1BA100 to /content/split/test\n",
            "Moved 1BB016 to /content/split/test\n",
            "Moved 1BA022 to /content/split/test\n",
            "Moved 1BA294 to /content/split/test\n",
            "Moved 1BC047 to /content/split/test\n",
            "Moved 1BA288 to /content/split/test\n",
            "Moved 1BB049 to /content/split/test\n",
            "Moved 1BA141 to /content/split/test\n",
            "Moved 1BA159 to /content/split/test\n",
            "Moved 1BC082 to /content/split/test\n",
            "Moved 1BA307 to /content/split/test\n",
            "Moved 1BC067 to /content/split/test\n",
            "Moved 1BA189 to /content/split/test\n",
            "Moved 1BA097 to /content/split/test\n",
            "Moved 1BA247 to /content/split/test\n",
            "Moved 1BC004 to /content/split/test\n",
            "Data split and moved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MESP(Structure)"
      ],
      "metadata": {
        "id": "oIKPMUDJ4tUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Swin Transformer Block (簡化版，適用於3D)\n",
        "class SwinTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, input_size):\n",
        "        super(SwinTransformerBlock, self).__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fc = nn.Linear(dim, dim)\n",
        "        self.window_size = input_size // 4  # 分割窗口，根據3D的尺寸調整\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, d, h, w = x.shape  # 3D 輸入\n",
        "        x = x.view(b, -1, c)  # 展平為序列\n",
        "        x = self.norm(x)\n",
        "        x = self.fc(x)\n",
        "        return x.view(b, c, d, h, w)  # 還原為 3D\n",
        "\n",
        "# MSEP 網路\n",
        "class MSEP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MSEP, self).__init__()\n",
        "        # Encoder 部分\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv3d(1, 64, kernel_size=3, stride=1, padding=1),  # 使用3D卷積\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1),  # 使用3D卷積\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Skip 連接部分 + RDSformer\n",
        "        self.skip = SwinTransformerBlock(128, input_size=160)\n",
        "        # Decoder 部分\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose3d(128, 64, kernel_size=3, stride=1, padding=1),  # 使用3D反卷積\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose3d(64, 1, kernel_size=3, stride=1, padding=1)  # 使用3D反卷積\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc = self.encoder(x)\n",
        "        skip = self.skip(enc)  # 加入 skip connection\n",
        "        dec = self.decoder(skip)\n",
        "        return dec\n",
        "\n",
        "# Initialize model\n",
        "model = MSEP()\n",
        "\n",
        "# Test the model with dummy data (e.g., [Batch Size, Channel, Depth, Height, Width])\n",
        "dummy_input = torch.randn(1, 1, 128, 128, 128)  # 假設數據大小是 [1, 1, 128, 128, 128]\n",
        "output = model(dummy_input)\n",
        "\n",
        "print(\"Output shape:\", output.shape)  # 應該返回符合預期的 3D 輸出\n"
      ],
      "metadata": {
        "id": "3BSm9S3z4D_6",
        "outputId": "0ba4234a-9347-4359-d793-be683a40ddd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 1, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training & Visualization"
      ],
      "metadata": {
        "id": "T7nNIv2z40an"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=True, delta=0.00005, path=\"checkpoint.pt\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): 容忍驗證損失未改善的次數 (default: 5)\n",
        "            verbose (bool): 是否打印相關資訊 (default: False)\n",
        "            delta (float): 最小改善幅度，只有超過此值才算改善 (default: 0)\n",
        "            path (str): 模型權重保存路徑 (default: \"checkpoint.pt\")\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float(\"inf\")\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        # 計算當前得分（驗證損失的負值，因為越小越好）\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        \"\"\"保存當前模型權重\"\"\"\n",
        "        if self.verbose:\n",
        "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...\")\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "def visualize_results(input_image, target_image, predicted_image, epoch, idx):\n",
        "    \"\"\"\n",
        "    視覺化輸入影像、目標影像與預測影像\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # 輸入影像\n",
        "    axes[0].imshow(input_image[0, 0, :, :, input_image.shape[4] // 2].cpu().detach().numpy(), cmap=\"gray\")\n",
        "    axes[0].set_title(\"Input (MR)\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # 目標影像3\n",
        "    axes[1].imshow(target_image[0, 0, :, :, target_image.shape[4] // 2].cpu().detach().numpy(), cmap=\"gray\")\n",
        "    axes[1].set_title(\"Target (CT)\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    # 預測影像\n",
        "    axes[2].imshow(predicted_image[0, 0, :, :, predicted_image.shape[4] // 2].cpu().detach().numpy(), cmap=\"gray\")\n",
        "    axes[2].set_title(\"Prediction (Generated CT)\")\n",
        "    axes[2].axis(\"off\")\n",
        "\n",
        "    plt.suptitle(f\"Epoch {epoch}, Batch {idx}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "rLxzMH7yYlYN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 優化器與損失函數\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)  # 學習率調整\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "# 訓練參數\n",
        "epochs = 50\n",
        "best_loss = float(\"inf\")\n",
        "\n",
        "train_path = \"/content/split/train\"\n",
        "valid_path = \"/content/split/validation\"\n",
        "test_path = \"/content/split/test\"\n",
        "\n",
        "train_dataset = MRCTDataset(train_path)\n",
        "valid_dataset = MRCTDataset(valid_path)\n",
        "test_dataset = MRCTDataset(test_path)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=6, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "for mr, ct in train_loader:\n",
        "    print(f\"MR shape: {mr.shape}, CT shape: {ct.shape}\")\n",
        "    break\n",
        "\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True, path=\"best_model.pt\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # 訓練階段\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "    for mr, ct in progress_bar:\n",
        "        mr, ct = mr.to(device), ct.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(mr)\n",
        "        loss = criterion(output, ct)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "    # if epoch % 5 == 0:  # 每 5 個 epoch 可視化一次\n",
        "    #     visualize_results(mr, ct, output, epoch, idx=1)\n",
        "    visualize_results(mr, ct, output, epoch, idx=1)\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Avg Train Loss: {avg_train_loss}\")\n",
        "\n",
        "    # 驗證階段\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for mr, ct in val_loader:\n",
        "            mr, ct = mr.to(device), ct.to(device)\n",
        "            output = model(mr)\n",
        "            loss = criterion(output, ct)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Avg Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "    # 調用 EarlyStopping\n",
        "    early_stopping(val_loss, model)\n",
        "\n",
        "    # 如果觸發 EarlyStopping，結束訓練\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered. Stopping training.\")\n",
        "        break\n",
        "\n",
        "    #保存最佳模型\n",
        "    # if avg_val_loss < best_loss:\n",
        "    #     best_loss = avg_val_loss\n",
        "    #     torch.save(model.state_dict(), \"best_model.pth\")\n",
        "    #     print(\"Best model saved!\")\n",
        "\n",
        "    # scheduler.step()\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_model.pt\"))"
      ],
      "metadata": {
        "id": "0R4-mT2P4wMd",
        "outputId": "1f846bae-789b-416c-891e-11c3e76f8fbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MR shape: torch.Size([6, 1, 128, 128, 128]), CT shape: torch.Size([6, 1, 128, 128, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50:   0%|          | 0/21 [00:28<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 14.75 GiB of which 5.53 GiB is free. Process 17538 has 9.21 GiB memory in use. Of the allocated memory 9.10 GiB is allocated by PyTorch, and 2.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e438dedc6ef0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-7152303778a5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 加入 skip connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1702\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 14.75 GiB of which 5.53 GiB is free. Process 17538 has 9.21 GiB memory in use. Of the allocated memory 9.10 GiB is allocated by PyTorch, and 2.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    # 訓練階段\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "    for mr, ct in progress_bar:\n",
        "        mr, ct = mr.to(device), ct.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(mr)\n",
        "        loss = criterion(output, ct)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "    visualize_results(mr, ct, output, epoch, 1)\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Avg Train Loss: {avg_train_loss}\")\n",
        "\n",
        "    # 驗證階段\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for mr, ct in val_loader:\n",
        "            mr, ct = mr.to(device), ct.to(device)\n",
        "            output = model(mr)\n",
        "            loss = criterion(output, ct)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Avg Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "    保存最佳模型\n",
        "    if avg_val_loss < best_loss:\n",
        "        best_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(\"Best model saved!\")\n",
        "\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "hLMd16Wd5zoq",
        "outputId": "f8720426-d586-4b04-da4a-46e29773b871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:   0%|          | 0/63 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"<ipython-input-3-9dfcba7c5583>\", line 47, in __getitem__\n    mr = self._resize_or_pad(mr, self.target_size)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-3-9dfcba7c5583>\", line 79, in _resize_or_pad\n    scale = [d / c for d, c in zip(desired_shape, current_shape)]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-3-9dfcba7c5583>\", line 79, in <listcomp>\n    scale = [d / c for d, c in zip(desired_shape, current_shape)]\n             ~~^~~\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-be01324afdf8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mmr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mmr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"<ipython-input-3-9dfcba7c5583>\", line 47, in __getitem__\n    mr = self._resize_or_pad(mr, self.target_size)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-3-9dfcba7c5583>\", line 79, in _resize_or_pad\n    scale = [d / c for d, c in zip(desired_shape, current_shape)]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-3-9dfcba7c5583>\", line 79, in <listcomp>\n    scale = [d / c for d, c in zip(desired_shape, current_shape)]\n             ~~^~~\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UyICao3H-Xkr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}