{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Laimo64/Laimo64/blob/main/AI_CW_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-kTcjeYesBM",
        "outputId": "d6a896a2-bf44-4bab-bd27-633d6ec3062e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1y8OFnIFYC_b_WIexKSog732_L78Pegxa\n",
            "From (redirected): https://drive.google.com/uc?id=1y8OFnIFYC_b_WIexKSog732_L78Pegxa&confirm=t&uuid=e02feaca-bf2a-4374-8b6a-1a3b12259baa\n",
            "To: /content/brain_small.zip\n",
            "100% 119M/119M [00:01<00:00, 118MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Small dataset\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1y8OFnIFYC_b_WIexKSog732_L78Pegxa/view?usp=drive_link\n",
        "!unzip -q brain_small.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full dataset\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1eSU-or72yvc3btOhfLO-edux5VBe_TrX/view?usp=sharing\n",
        "!unzip -q brain.zip"
      ],
      "metadata": {
        "id": "MAGAQrhJK9S3",
        "outputId": "f9b133c0-8efa-4345-cd3b-344e34d0b1a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/gdown\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gdown/__main__.py\", line 172, in main\n",
            "    download(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gdown/download.py\", line 202, in download\n",
            "    res = sess.get(url, stream=True, verify=verify)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 602, in get\n",
            "    return self.request(\"GET\", url, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 579, in request\n",
            "    settings = self.merge_environment_settings(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 760, in merge_environment_settings\n",
            "    env_proxies = get_environ_proxies(url, no_proxy=no_proxy)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/utils.py\", line 832, in get_environ_proxies\n",
            "    if should_bypass_proxies(url, no_proxy=no_proxy):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/utils.py\", line 816, in should_bypass_proxies\n",
            "    bypass = proxy_bypass(parsed.hostname)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/urllib/request.py\", line 2538, in proxy_bypass_environment\n",
            "    proxies = getproxies_environment()\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/urllib/request.py\", line 2521, in getproxies_environment\n",
            "    for name, value in os.environ.items():\n",
            "  File \"<frozen _collections_abc>\", line 860, in __iter__\n",
            "  File \"<frozen os>\", line 701, in __iter__\n",
            "  File \"<frozen os>\", line 761, in decode\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "unzip:  cannot find or open brain.zip, brain.zip.zip or brain.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# data_path = \"/content/brain\"\n",
        "# output_root = \"/content/split\"  # 輸出目錄\n",
        "data_path = \"/content/small\"\n",
        "output_root = \"/content/split\"  # 輸出目錄\n",
        "\n",
        "# 創建輸出資料夾\n",
        "os.makedirs(output_root, exist_ok=True)\n",
        "os.makedirs(os.path.join(output_root, \"train\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_root, \"validation\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_root, \"test\"), exist_ok=True)\n",
        "\n",
        "# 獲取所有樣本資料夾名稱\n",
        "samples = [name for name in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, name))]\n",
        "\n",
        "# 按照 70:15:15 的比例分割\n",
        "train_samples, test_samples = train_test_split(samples, test_size=0.3, random_state=42)\n",
        "validation_samples, test_samples = train_test_split(test_samples, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Total samples: {len(samples)}\")\n",
        "print(f\"Train samples: {len(train_samples)}, Validation samples: {len(validation_samples)}, Test samples: {len(test_samples)}\")\n",
        "\n",
        "# 定義拷貝函數\n",
        "def move_samples(samples, output_dir):\n",
        "    for sample in samples:\n",
        "        src_path = os.path.join(data_path, sample)  # 原始路徑\n",
        "        dst_path = os.path.join(output_dir, sample)  # 目標路徑\n",
        "        if os.path.exists(dst_path):\n",
        "            print(f\"Sample {sample} already exists in {output_dir}, skipping.\")\n",
        "            continue\n",
        "        shutil.copytree(src_path, dst_path)  # 拷貝整個資料夾\n",
        "\n",
        "# 將樣本移動到各自的資料夾\n",
        "move_samples(train_samples, os.path.join(output_root, \"train\"))\n",
        "move_samples(validation_samples, os.path.join(output_root, \"validation\"))\n",
        "move_samples(test_samples, os.path.join(output_root, \"test\"))\n",
        "\n",
        "print(\"Data split and moved successfully!\")\n"
      ],
      "metadata": {
        "id": "nAIGDZ03SICL",
        "outputId": "85f355ab-2480-4cef-b061-8201944478cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 4\n",
            "Train samples: 2, Validation samples: 1, Test samples: 1\n",
            "Sample 1BA012 already exists in /content/split/train, skipping.\n",
            "Sample 1BA005 already exists in /content/split/train, skipping.\n",
            "Sample 1BA014 already exists in /content/split/validation, skipping.\n",
            "Sample 1BA001 already exists in /content/split/test, skipping.\n",
            "Data split and moved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "class MRCTDataset(Dataset):\n",
        "    def __init__(self, data_dir, target_size=(128, 128, 128)):\n",
        "        \"\"\"\n",
        "        初始化數據集\n",
        "        Args:\n",
        "            data_dir (str): MRI 和 CT 數據的根目錄。\n",
        "            target_size (tuple): 將 MRI 和 CT 影像調整為的固定尺寸。\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.target_size = target_size\n",
        "        self.samples = [\n",
        "            os.path.join(root)\n",
        "            for root, _, files in os.walk(data_dir)\n",
        "            if \"mr.nii.gz\" in files and \"ct.nii.gz\" in files\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        根據索引返回一組 MR 和 CT 影像。\n",
        "        Args:\n",
        "            idx (int): 數據的索引。\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]: MR 和 CT 的張量形式。\n",
        "        \"\"\"\n",
        "        sample_path = self.samples[idx]\n",
        "\n",
        "        # 加載 MRI 和 CT 影像\n",
        "        mr = nib.load(os.path.join(sample_path, \"mr.nii.gz\")).get_fdata()\n",
        "        ct = nib.load(os.path.join(sample_path, \"ct.nii.gz\")).get_fdata()\n",
        "\n",
        "        # Z-score 標準化\n",
        "        mr = self._normalize(mr)\n",
        "        ct = self._normalize(ct)\n",
        "\n",
        "        # 調整或填充影像大小\n",
        "        mr = self._resize_or_pad(mr, self.target_size)\n",
        "        ct = self._resize_or_pad(ct, self.target_size)\n",
        "\n",
        "        # 轉換為 PyTorch 張量並增加通道維度\n",
        "        mr = torch.tensor(mr, dtype=torch.float32).unsqueeze(0)  # (1, D, H, W)\n",
        "        ct = torch.tensor(ct, dtype=torch.float32).unsqueeze(0)  # (1, D, H, W)\n",
        "\n",
        "        return mr, ct\n",
        "\n",
        "    def _normalize(self, image):\n",
        "        \"\"\"\n",
        "        Z-score 標準化影像數據。\n",
        "        Args:\n",
        "            image (np.ndarray): 輸入影像。\n",
        "        Returns:\n",
        "            np.ndarray: 標準化的影像。\n",
        "        \"\"\"\n",
        "        if np.std(image) != 0:\n",
        "            return (image - np.mean(image)) / np.std(image)\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "    def _resize_or_pad(self, image, desired_shape):\n",
        "        \"\"\"\n",
        "        調整影像大小或填充至固定大小。\n",
        "        Args:\n",
        "            image (np.ndarray): 輸入影像。\n",
        "            desired_shape (tuple): 目標大小。\n",
        "        Returns:\n",
        "            np.ndarray: 調整或填充後的影像。\n",
        "        \"\"\"\n",
        "        current_shape = image.shape\n",
        "        scale = [d / c for d, c in zip(desired_shape, current_shape)]\n",
        "        resized_image = zoom(image, scale, order=1)  # 調整大小\n",
        "\n",
        "        # 填充影像至目標大小\n",
        "        padded_image = np.zeros(desired_shape, dtype=resized_image.dtype)\n",
        "        pad_slices = tuple(slice(0, min(dim, resized_image.shape[i])) for i, dim in enumerate(desired_shape))\n",
        "        padded_image[pad_slices] = resized_image[:desired_shape[0], :desired_shape[1], :desired_shape[2]]\n",
        "\n",
        "        return padded_image\n"
      ],
      "metadata": {
        "id": "a3ZVGyaZ6roW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EarlyStopping & Visualization"
      ],
      "metadata": {
        "id": "ETuq9nOCbk6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=True, delta=0.00005, path=\"checkpoint.pth\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): 容忍驗證損失未改善的次數 (default: 5)\n",
        "            verbose (bool): 是否打印相關資訊 (default: False)\n",
        "            delta (float): 最小改善幅度，只有超過此值才算改善 (default: 0)\n",
        "            path (str): 模型權重保存路徑 (default: \"checkpoint.pt\")\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float(\"inf\")\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        # 計算當前得分（驗證損失的負值，因為越小越好）\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        \"\"\"保存當前模型權重\"\"\"\n",
        "        if self.verbose:\n",
        "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...\")\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "def visualize_results(input_image, target_image, predicted_image, epoch, idx):\n",
        "    \"\"\"\n",
        "    視覺化輸入影像、目標影像與預測影像\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # 輸入影像\n",
        "    axes[0].imshow(input_image[0, 0, :, :, input_image.shape[4] // 2].cpu().detach().numpy(), cmap=\"gray\")\n",
        "    axes[0].set_title(\"Input (MR)\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # 目標影像3\n",
        "    axes[1].imshow(target_image[0, 0, :, :, target_image.shape[4] // 2].cpu().detach().numpy(), cmap=\"gray\")\n",
        "    axes[1].set_title(\"Target (CT)\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    # 預測影像\n",
        "    axes[2].imshow(predicted_image[0, 0, :, :, predicted_image.shape[4] // 2].cpu().detach().numpy(), cmap=\"gray\")\n",
        "    axes[2].set_title(\"Prediction (Generated CT)\")\n",
        "    axes[2].axis(\"off\")\n",
        "\n",
        "    plt.suptitle(f\"Epoch {epoch}, Batch {idx}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "zHHe2l-xe0B9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAN model"
      ],
      "metadata": {
        "id": "n9FBZzgvbsng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import nibabel as nib\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch import nn, optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torchvision.transforms import Normalize\n",
        "# from scipy.ndimage import zoom\n",
        "\n",
        "\n",
        "# # Data preprocessing and dataset class\n",
        "# class MRCTDataset(Dataset):\n",
        "#     def __init__(self, data_dir, target_size=(128, 128, 128)):\n",
        "#         self.data_dir = data_dir\n",
        "#         self.target_size = target_size\n",
        "#         self.samples = [\n",
        "#             os.path.join(root)\n",
        "#             for root, _, files in os.walk(data_dir)\n",
        "#             if \"mr.nii.gz\" in files and \"ct.nii.gz\" in files and \"mask.nii.gz\" in files\n",
        "#         ]\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.samples)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         sample_path = self.samples[idx]\n",
        "#         mr = nib.load(os.path.join(sample_path, \"mr.nii.gz\")).get_fdata()\n",
        "#         ct = nib.load(os.path.join(sample_path, \"ct.nii.gz\")).get_fdata()\n",
        "\n",
        "#         mr = self._normalize(mr)\n",
        "#         ct = self._normalize(ct)\n",
        "\n",
        "#         mr = self._resize_or_pad(mr, self.target_size)\n",
        "#         ct = self._resize_or_pad(ct, self.target_size)\n",
        "\n",
        "#         mr = torch.tensor(mr, dtype=torch.float32).unsqueeze(0)\n",
        "#         ct = torch.tensor(ct, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "#         return mr, ct\n",
        "\n",
        "#     def _normalize(self, image):\n",
        "#         return (image - np.mean(image)) / (np.std(image) + 1e-5)\n",
        "\n",
        "#     def _resize_or_pad(self, image, target_size):\n",
        "#         zoom_factors = [t / s for t, s in zip(target_size, image.shape)]\n",
        "#         return zoom(image, zoom_factors, order=1)\n",
        "\n",
        "# # Generator and Discriminator Networks\n",
        "# class Generator(nn.Module):   # to transform MR to CT using 3D CNN\n",
        "#     def __init__(self):\n",
        "#         super(Generator, self).__init__()\n",
        "#         self.model = nn.Sequential(\n",
        "#             nn.Conv3d(1, 64, kernel_size=4, stride=2, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv3d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "#             nn.BatchNorm3d(128),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.ConvTranspose3d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "#             nn.BatchNorm3d(64),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.ConvTranspose3d(64, 1, kernel_size=4, stride=2, padding=1),\n",
        "#             nn.Tanh()   # 使範圍在[-1, 1]\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.model(x)\n",
        "\n",
        "# class Discriminator(nn.Module):  # to tell whether the input image is the ground truth from the dataset or generated by the generator\n",
        "#     def __init__(self):\n",
        "#         super(Discriminator, self).__init__()\n",
        "#         self.model = nn.Sequential(\n",
        "#             nn.Conv3d(1, 64, kernel_size=4, stride=2, padding=1),\n",
        "#             nn.LeakyReLU(0.2, inplace=True),\n",
        "#             nn.Conv3d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "#             nn.BatchNorm3d(128),\n",
        "#             nn.LeakyReLU(0.2, inplace=True),\n",
        "#             nn.Conv3d(128, 1, kernel_size=4, stride=2, padding=1),\n",
        "#             nn.Sigmoid()       # 使範圍在[0, 1] 希望判斷真實影像時越接近1，希望判斷生成影像時越接近0\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.model(x)"
      ],
      "metadata": {
        "id": "vD9JhslHbv7n"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Swin Transformer Block (簡化版，適用於3D)\n",
        "class SwinTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, input_size):\n",
        "        super(SwinTransformerBlock, self).__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fc = nn.Linear(dim, dim)\n",
        "        self.window_size = input_size // 4  # 分割窗口，根據3D的尺寸調整\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, d, h, w = x.shape  # 3D 輸入\n",
        "        x = x.view(b, -1, c)  # 展平為序列\n",
        "        x = self.norm(x)\n",
        "        x = self.fc(x)\n",
        "        return x.view(b, c, d, h, w)  # 還原為 3D\n",
        "\n",
        "# MSEP 網路\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        # Encoder 部分\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv3d(1, 64, kernel_size=3, stride=1, padding=1),  # 使用3D卷積\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1),  # 使用3D卷積\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Skip 連接部分 + RDSformer\n",
        "        self.skip = SwinTransformerBlock(128, input_size=160)\n",
        "        # Decoder 部分\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose3d(128, 64, kernel_size=3, stride=1, padding=1),  # 使用3D反卷積\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose3d(64, 1, kernel_size=3, stride=1, padding=1)  # 使用3D反卷積\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc = self.encoder(x)\n",
        "        skip = self.skip(enc)  # 加入 skip connection\n",
        "        dec = self.decoder(skip)\n",
        "        return dec\n",
        "\n",
        "# Initialize model\n",
        "model = Generator()\n",
        "\n",
        "# Test the model with dummy data (e.g., [Batch Size, Channel, Depth, Height, Width])\n",
        "dummy_input = torch.randn(1, 1, 128, 128, 128)  # 假設數據大小是 [1, 1, 128, 128, 128]\n",
        "output = model(dummy_input)\n",
        "\n",
        "print(\"Output shape:\", output.shape)  # 應該返回符合預期的 3D 輸出\n",
        "\n",
        "class Discriminator(nn.Module):  # to tell whether the input image is the ground truth from the dataset or generated by the generator\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv3d(1, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv3d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm3d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv3d(128, 1, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Sigmoid()       # 使範圍在[0, 1] 希望判斷真實影像時越接近1，希望判斷生成影像時越接近0\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "decKpSPh6A03",
        "outputId": "919214b2-829a-4c37-dee0-56fcc0dc43f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 1, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkpoint"
      ],
      "metadata": {
        "id": "SbSlJ2b_xvTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def save_checkpoint(self, val_loss, model):\n",
        "#     # 儲存模型和優化器狀態\n",
        "#     torch.save({\n",
        "#         'epoch': epoch,  # 當前 epoch\n",
        "#         'model_state_dict': model.state_dict(),\n",
        "#         'optimizer_state_dict': optimizer.state_dict(),\n",
        "#         'val_loss': val_loss,\n",
        "#     }, self.path)\n",
        "#     print(f\"Checkpoint saved at {self.path}\")\n",
        "\n",
        "def load_checkpoint(model, optimizer, path=\"checkpoint.pth\"):\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    val_loss = checkpoint['val_loss']\n",
        "    print(f\"Checkpoint loaded from {path}\")\n",
        "    return model, optimizer, epoch, val_loss\n"
      ],
      "metadata": {
        "id": "wob1UYX2xyWm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import torch.nn.functional as F\n",
        "import torch._dynamo\n",
        "\n",
        "\n",
        "# L2 Loss 來代替 BCELoss (Least Squares GAN (LSGAN))\n",
        "def lsgan_loss(pred, target):\n",
        "    return 0.5 * torch.mean((pred - target) ** 2)\n",
        "\n",
        "\n",
        "def calculate_ssim(fake, real):\n",
        "    # 確保數據形狀為 (N, C, D, H, W)\n",
        "    fake_np = fake.squeeze(1).cpu().numpy()  # 去掉單通道 (C=1)\n",
        "    real_np = real.squeeze(1).cpu().numpy()\n",
        "\n",
        "    ssim_values = []\n",
        "    for i in range(fake_np.shape[0]):  # 遍歷批次\n",
        "        for d in range(fake_np.shape[1]):  # 遍歷深度維度\n",
        "            ssim_value = ssim(\n",
        "                fake_np[i, d], real_np[i, d], data_range=real_np[i, d].max() - real_np[i, d].min()\n",
        "            )\n",
        "            ssim_values.append(ssim_value)\n",
        "\n",
        "    return sum(ssim_values) / len(ssim_values)  # 返回平均 SSIM\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train_gan(data_loader, val_loader, generator, discriminator, g_optimizer, d_optimizer, criterion, epochs=100, device='cuda'):\n",
        "    generator.to(device)\n",
        "    discriminator.to(device)\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=5, verbose=True, delta=0.00005, path=\"checkpoint.pth\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i, (mr, ct) in enumerate(data_loader):\n",
        "            mr, ct = mr.to(device), ct.to(device)\n",
        "\n",
        "            # Train Discriminator\n",
        "            real_output = discriminator(ct)\n",
        "            fake_ct = generator(mr)\n",
        "            fake_ct = torch.sigmoid(fake_ct)  # 保證範圍在 [0, 1]\n",
        "            fake_output = discriminator(fake_ct.detach())\n",
        "\n",
        "            # LSGAN 判別器損失\n",
        "            d_loss_real = lsgan_loss(real_output, torch.ones_like(real_output))  # 真實樣本對應1\n",
        "            d_loss_fake = lsgan_loss(fake_output, torch.zeros_like(fake_output))  # 假樣本對應0\n",
        "            d_loss = (d_loss_real + d_loss_fake) / 2\n",
        "\n",
        "            d_optimizer.zero_grad()\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            # Train Generator\n",
        "            fake_output = discriminator(fake_ct)\n",
        "            # LSGAN 生成器損失\n",
        "            g_loss = lsgan_loss(fake_output, torch.ones_like(fake_output))  # 生成器希望判別器給 1\n",
        "\n",
        "            g_optimizer.zero_grad()\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(data_loader)}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "        # 可視化結果：每 5 個 epoch 執行一次\n",
        "        random_idx = torch.randint(0, len(mr), (1,)).item()\n",
        "        visualize_results(mr[random_idx:random_idx+1], ct[random_idx:random_idx+1], fake_ct[random_idx:random_idx+1], epoch, idx=4)\n",
        "\n",
        "        # validation\n",
        "        generator.eval()\n",
        "        val_loss = 0.0\n",
        "        ssim_score = 0.0  # Example for SSIM metric\n",
        "        with torch.no_grad():\n",
        "            for mr, ct in val_loader:\n",
        "                mr, ct = mr.to(device), ct.to(device)\n",
        "                fake_ct = generator(mr)\n",
        "                loss = F.mse_loss(fake_ct, ct)  # 使用 MSE Loss 或 SSIM 作為附加度量\n",
        "                val_loss += loss.item()\n",
        "                ssim_score += calculate_ssim(fake_ct, ct)\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        avg_ssim = ssim_score / len(val_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Avg Validation Loss: {avg_val_loss:.4f}, SSIM: {avg_ssim:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        early_stopping(avg_val_loss + (1 - avg_ssim), generator)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered. Stopping training.\")\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "# Main Script\n",
        "if __name__ == \"__main__\":\n",
        "    train_data = \"/content/split/train\"\n",
        "    val_data = \"/content/split/validation\"\n",
        "    test_data = \"/content/split/test\"\n",
        "    train_set = MRCTDataset(train_data)\n",
        "    val_set = MRCTDataset(val_data)\n",
        "    val_loader = DataLoader(val_set, batch_size=2, shuffle=False)\n",
        "    data_loader = DataLoader(train_set, batch_size=2, shuffle=True)\n",
        "\n",
        "    generator = Generator()\n",
        "    discriminator = Discriminator()\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    g_optimizer = optim.Adam(generator.parameters(), lr=0.0005)\n",
        "    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0005)\n",
        "\n",
        "    # 嘗試從檢查點加載模型\n",
        "    start_epoch = 0\n",
        "    if os.path.exists(\"checkpoint.pth\"):\n",
        "        generator, g_optimizer, start_epoch, _ = load_checkpoint(generator, g_optimizer)\n",
        "        discriminator, d_optimizer, _, _ = load_checkpoint(discriminator, d_optimizer)\n",
        "\n",
        "    train_gan(data_loader, val_loader, generator, discriminator, g_optimizer, d_optimizer, criterion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "12V2y3jXevHy",
        "outputId": "326e4873-0eef-4fa7-e2b2-0c98c45da9aa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'torch._dynamo' has no attribute 'config' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-c9417a029a40>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructural_similarity\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mssim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallback_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traceback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCapturedTraceback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_traceback_short\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbytecode_analysis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mremove_dead_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_pointless_jumps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m from .bytecode_transformation import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/trace_rules.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   3216\u001b[0m         \u001b[0;34m\"torch.distributed._composable.replicate\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3217\u001b[0m     }\n\u001b[0;32m-> 3218\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_fsdp_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3219\u001b[0m         \u001b[0mLEGACY_MOD_INLINELIST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch.distributed._composable.fsdp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torch._dynamo' has no attribute 'config' (most likely due to a circular import)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Path to the checkpoint file\n",
        "checkpoint_path = 'checkpoint.pth'\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "# Print the keys to see what data is inside the checkpoint\n",
        "print(\"Checkpoint contains the following keys:\")\n",
        "for key in checkpoint.keys():\n",
        "    print(key)\n",
        "\n",
        "# Optionally, inspect the contents of specific keys (e.g., if you want to see 'epoch' or 'model_state_dict')\n",
        "print(\"\\nExample content of 'epoch':\", checkpoint.get('epoch', 'Not Found'))\n",
        "print(\"Example content of 'val_loss':\", checkpoint.get('val_loss', 'Not Found'))\n"
      ],
      "metadata": {
        "id": "SKk9JmjnzJdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oij-h7Ju2RF6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}